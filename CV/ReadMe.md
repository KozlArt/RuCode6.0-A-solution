## Подготовка датасета
Изначальный датасет был обогащен открытыми данными [здесь](https://www.kaggle.com/datasets/landrykezebou/vcor-vehicle-color-recognition-dataset), [здесь](https://arxiv.org/pdf/1510.07391.pdf)([файл](https://www.reddit.com/r/datasets/comments/fnivwh/comment/i5bfjiq/?utm_source=share&utm_medium=web2x&context=3)).
Часть фотографий были вручную удалены. Часть были затемнены. Была проведена аугментация данных до 3500 фотографий на каждую категорию.
Подробнее можно посмотреть в ноутбуке augmentation.ipynb

## Обучение модели
Дообучена resnet34 20 эпох на V100 1 час 30 минут. 

## Датасет веса, файлы аннотаций
Файлы находятся [здесь](https://drive.google.com/drive/folders/16dJ38ke-tRP9o5-JiVaUqfr1cxxUAFvk?usp=sharing).

## Как запустить решение

Можно скачать файлы аннотации с гугл диска или сделать их самостоятельно

Чтобы запустить тренировочный или валидационный ноутбук, предварительно необходимо создать файл-аннотацию, запустив скрипт make_annotation_file.py, в котором:
- выбрать режим train/test;
- назвать файл;
- выбрать базовую директорию, где хранятся изоюражения.

Файл-аннотацию необходимо создавать как для трейн, так и тестовой частей данных.

Скрипт можно запускать как в так и вне jupyter ноутбука. Запуск внутри среды jupyter, таким образом: 
```
import src.make_annotation_file as annot
annot.make()
```
Директорию "src" необходимо расположить рядом с ноутбуками, чтобы не получить ошибки импорта самописных модулей.
Если ошибка всё же возникает, необходимо содержимое src выгрузить в общую с ноутбуками директорию. Тогда в ячейке импорта модулей, необходимо заменить все вхождения
src.module на module:

- import src.model_fitting as model_fitting -> import model_fitting
- from src.processing import * -> from processing import *

Для формирования сабмита, запустите fitting.ipynb, для валидации сети на данных - validation.ipynb.  
EDA.ipynb - максимально краткий EDA анализ исходных данных.
